{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a0d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/refs/heads/master/data/2021/2021-07-20/drought.csv\"\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9406c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert dates to datetime and extract year for grouping\n",
    "data['valid_start'] = pd.to_datetime(data['valid_start'])\n",
    "data['year'] = data['valid_start'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5960f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example lookup dictionary with some state abbreviations and their FIPS codes\n",
    "state_fips_mapping = {\n",
    "    'AK': 2, 'AL': 1, 'AR': 5, 'AZ': 4, 'CA': 6, 'CO': 8, 'CT': 9, 'DE': 10, \n",
    "    'FL': 12, 'GA': 13, 'HI': 15, 'ID': 16, 'IL': 17, 'IN': 18, 'IA': 19,\n",
    "    # Continue with all states\n",
    "    'WI': 55, 'WV': 54, 'WY': 56\n",
    "}\n",
    "\n",
    "# Step 1: Map state abbreviations to FIPS codes using the lookup dictionary\n",
    "data['state_fips'] = data['state_abb'].map(state_fips_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94263e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['state_fips', 'drought_lvl', 'area_pct'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94775e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare data for \"All Years,\" \"Decade 1,\" and \"Decade 2\" aggregation\n",
    "# Aggregate by state and drought level for each time range\n",
    "\n",
    "# \"All Years\" (2001-2021)\n",
    "all_years_summary = data.groupby(['state_fips', 'drought_lvl']).agg({\n",
    "    'area_pct': 'mean'\n",
    "}).reset_index()\n",
    "all_years_summary['time_range'] = '2001-2021'\n",
    "\n",
    "# \"Decade 1\" (2001-2010)\n",
    "decade_1 = data[(data['year'] >= 2001) & (data['year'] <= 2010)]\n",
    "decade_1_summary = decade_1.groupby(['state_fips', 'drought_lvl']).agg({\n",
    "    'area_pct': 'mean'\n",
    "}).reset_index()\n",
    "decade_1_summary['time_range'] = '2001-2010'\n",
    "\n",
    "# \"Decade 2\" (2011-2021)\n",
    "decade_2 = data[(data['year'] >= 2011) & (data['year'] <= 2021)]\n",
    "decade_2_summary = decade_2.groupby(['state_fips', 'drought_lvl']).agg({\n",
    "    'area_pct': 'mean'\n",
    "}).reset_index()\n",
    "decade_2_summary['time_range'] = '2011-2021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac52f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine summaries for filtering in Vega-Lite\n",
    "combined_summary = pd.concat([all_years_summary, decade_1_summary, decade_2_summary], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80aebed4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'time_range'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 5: Prepare data for pie chart (proportions of drought levels)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pie_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrought_lvl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_range\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39magg({\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea_pct\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m })\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8400\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8403\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   8404\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m   8405\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   8406\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   8407\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[0;32m   8408\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   8409\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[0;32m   8410\u001b[0m     squeeze\u001b[38;5;241m=\u001b[39msqueeze,\n\u001b[0;32m   8411\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m   8412\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[0;32m   8413\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[0;32m    966\u001b[0m         obj,\n\u001b[0;32m    967\u001b[0m         keys,\n\u001b[0;32m    968\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    969\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m    970\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    971\u001b[0m         observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m    972\u001b[0m         mutated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmutated,\n\u001b[0;32m    973\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'time_range'"
     ]
    }
   ],
   "source": [
    "# Step 5: Prepare data for pie chart (proportions of drought levels)\n",
    "pie_data = data.groupby(['drought_lvl', 'time_range']).agg({\n",
    "    'area_pct': 'mean'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cd68e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  drought_lvl time_range   area_pct\n",
      "0          D0  2001-2010  17.562799\n",
      "1          D0  2011-2021  16.241132\n",
      "2          D1  2001-2010  10.690936\n",
      "3          D1  2011-2021  10.252975\n",
      "4          D2  2001-2010   8.253439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swaro\\AppData\\Local\\Temp\\ipykernel_1440\\3076689102.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['time_range'] = '2001-2021'\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter out rows where drought level is \"None\"\n",
    "data = data[data['drought_lvl'] != 'None']\n",
    "\n",
    "# Step 2: Add the 'time_range' column to the main dataset based on year ranges\n",
    "\n",
    "# \"All Years\" (2001-2021)\n",
    "data['time_range'] = '2001-2021'\n",
    "\n",
    "# Set time_range for Decade 1 (2001-2010)\n",
    "data.loc[(data['year'] >= 2001) & (data['year'] <= 2010), 'time_range'] = '2001-2010'\n",
    "\n",
    "# Set time_range for Decade 2 (2011-2021)\n",
    "data.loc[(data['year'] >= 2011) & (data['year'] <= 2021), 'time_range'] = '2011-2021'\n",
    "\n",
    "# Step 3: Now proceed with preparing pie chart data\n",
    "pie_data = data.groupby(['drought_lvl', 'time_range']).agg({\n",
    "    'area_pct': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Verify the structure of pie_data\n",
    "print(pie_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9e52a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Prepare data for trend line chart (2001-2021 yearly trends)\n",
    "trend_data = data.groupby(['year', 'drought_lvl']).agg({\n",
    "    'area_pct': 'mean'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eaab186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export datasets for visualization\n",
    "combined_summary.to_csv('combined_drought_summary.csv', index=False)\n",
    "pie_data.to_csv('pie_drought_data.csv', index=False)\n",
    "trend_data.to_csv('trend_drought_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1732806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: state_fips, dtype: int32\n",
      "Data has been exported as 'cleaned_drought_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/SwaroopG28/Drought-Dataset---IV-Assignment/refs/heads/main/combined_drought_summary.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Convert 'state_fips' column to integer\n",
    "data['state_fips'] = data['state_fips'].fillna(0).astype(int)\n",
    "\n",
    "# Check to ensure conversion\n",
    "print(data['state_fips'].head())\n",
    "\n",
    "# Export to a CSV file on your local machine\n",
    "data.to_csv(\"cleaned_drought_data.csv\", index=False)\n",
    "print(\"Data has been exported as 'cleaned_drought_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf28979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
